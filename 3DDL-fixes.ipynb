{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import trimesh\n",
    "import k3d\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# !conda install -c conda-forge pyembree\n",
    "# !conda install -c conda-forge igl\n",
    "# !pip install Cython\n",
    "# !pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import trimesh\n",
    "import k3d\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from geometry.model import Model, combine_observations, get_mesh\n",
    "from geometry.utils.visualisation import illustrate_points, illustrate_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684849cc400f45cc8b5190d044f9a3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n",
      "/opt/conda/lib/python3.7/site-packages/traittypes/traittypes.py:101: UserWarning: Given trait value dtype \"float64\" does not match required type \"float32\". A coerced copy has been created.\n",
      "  np.dtype(self.dtype).name))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-30b1dabedaf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mobservations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mview_point_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_POINTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview_point_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0millustrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.04\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/NextBestViewRL/geometry/model.py\u001b[0m in \u001b[0;36mget_observation\u001b[0;34m(self, view_point_idx)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate_to_view_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraycast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/NextBestViewRL/geometry/model.py\u001b[0m in \u001b[0;36mraycast\u001b[0;34m(self, visibility_eps)\u001b[0m\n\u001b[1;32m    132\u001b[0m          \u001b[0mnormals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m          \u001b[0mmesh_vertex_indexes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m          mesh_face_indexes) = self.raycaster.get_image(self.mesh)\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mnoisy_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_direction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/NextBestViewRL/geometry/utils/raycasting.py\u001b[0m in \u001b[0;36mget_image\u001b[0;34m(self, mesh)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# get a point cloud with corresponding indexes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         mesh_face_indexes, ray_indexes, points = ray_cast_mesh(\n\u001b[0;32m---> 25\u001b[0;31m             mesh, self.rays_origins, self.rays_directions)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# extract normals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/NextBestViewRL/geometry/utils/raycasting.py\u001b[0m in \u001b[0;36mray_cast_mesh\u001b[0;34m(mesh, rays_origins, ray_directions)\u001b[0m\n\u001b[1;32m    103\u001b[0m     index_triangles, index_ray, point_cloud = intersector.intersects_id(\n\u001b[1;32m    104\u001b[0m         \u001b[0mray_origins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrays_origins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mray_directions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mray_directions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         multiple_hits=False, return_locations=True)\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindex_triangles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_ray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoint_cloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/trimesh/constants.py\u001b[0m in \u001b[0;36mtimed\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtimed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         log.debug('%s executed in %.4f seconds.',\n\u001b[1;32m    136\u001b[0m                   \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_POINTS = 10\n",
    "\n",
    "\n",
    "plot = k3d.plot(name='points')\n",
    "plot.display()\n",
    "\n",
    "model = Model(\"./data/Dude.obj\")\n",
    "model.generate_view_points(NUM_POINTS)\n",
    "\n",
    "observations = []\n",
    "for view_point_idx in range(NUM_POINTS):\n",
    "    observation = model.get_observation(view_point_idx)\n",
    "    \n",
    "    plot = observation.illustrate(plot, size=0.04)\n",
    "    plot = illustrate_points([model.get_point(view_point_idx)], size=1.0, plot=plot)\n",
    "    sleep(2)\n",
    "    \n",
    "    observations.append(observation)\n",
    "    \n",
    "combined_observation = combine_observations(observations)\n",
    "reconstructed_vertices, reconstructed_faces = get_mesh(combined_observation)\n",
    "\n",
    "loss = model.surface_similarity(reconstructed_vertices, reconstructed_faces)\n",
    "print(loss)\n",
    "\n",
    "illustrate_mesh(reconstructed_vertices, reconstructed_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "from pypoisson import poisson_reconstruction\n",
    "\n",
    "\n",
    "MAX_POINS_CNT = int(1e6)\n",
    "VIEW_POINTS_CNT = 1000\n",
    "MEMORY_SIZE = 10\n",
    "\n",
    "\n",
    "class EnvError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Environment(gym.Env):\n",
    "    def __init__(self, number_of_view_points=VIEW_POINTS_CNT, illustrate=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.number_of_view_points = number_of_view_points\n",
    "        self.illustrate = illustrate\n",
    "\n",
    "        self.action_space = spaces.Discrete(number_of_view_points)\n",
    "        \n",
    "        image_size = 512 # model.raycaster.resolution_image\n",
    "\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'points':  spaces.Box(-np.inf, np.inf, (MAX_POINS_CNT, 3), dtype=np.float32),\n",
    "            # 'normals': spaces.Box(-np.inf, np.inf, (MAX_POINS_CNT, 3), dtype=np.float32),\n",
    "            # 'vertex_indexes': spaces.Box(-np.inf, np.inf, (MAX_POINS_CNT, 3), dtype=np.int32),\n",
    "            # 'face_indexes': spaces.Box(-np.inf, np.inf, (MAX_POINS_CNT, 3), dtype=np.int32),\n",
    "            'depth_map': spaces.Box(-np.inf, np.inf, (MEMORY_SIZE, image_size, image_size), dtype=np.float32),\n",
    "            # 'normals_image': spaces.Box(-np.inf, np.inf, (VIEW_POINTS_CNT, image_size, image_size), dtype=np.float32)\n",
    "        })\n",
    "\n",
    "        self._similarity_threshold = 1 - 1e-3\n",
    "        self._reconstruction_depth = 10\n",
    "\n",
    "        self.model = None\n",
    "        self.plot = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment for new episode.\n",
    "        Randomly (or not) generate CAD model for this episode.\n",
    "        \"\"\"\n",
    "        self.model = Model(\"./data/Dude.obj\")\n",
    "        self.model.generate_view_points(self.number_of_view_points)\n",
    "        \n",
    "        if self.illustrate:\n",
    "            self.model.illustrate().display()\n",
    "        \n",
    "        init_state = self.action_space.sample()\n",
    "        observation = self.model.get_observation(init_state)\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Get new observation from current position (action), count step reward, decide whether to stop.\n",
    "        Args:\n",
    "            action: int\n",
    "        return: \n",
    "            next_state: List[List[List[int, int, int]]]\n",
    "            reward: float\n",
    "            done: bool\n",
    "            info: Tuple\n",
    "        \"\"\"\n",
    "        assert env.action_space.contains(action)\n",
    "        observation = self.model.get_observation(action)\n",
    "\n",
    "        reward = self.step_reward(observation)\n",
    "        done = reward >= self._similarity_threshold\n",
    "\n",
    "        return observation, reward, done, {}\n",
    "    \n",
    "    def render(self, action, observation, plot=None):\n",
    "        if plot is None:\n",
    "            plot = self.plot\n",
    "            \n",
    "        plot = illustrate_points(\n",
    "           [self.model.get_point(action)], size=0.5, plot=plot)\n",
    "        \n",
    "        plot = observation.illustrate(plot, size=0.03)\n",
    "        return plot\n",
    "    \n",
    "    def step_reward(self, observation):\n",
    "        return self.model.observation_similarity(observation)\n",
    "    \n",
    "    def final_reward(self, observation):\n",
    "        vertices, faces = self._get_mesh(observation)\n",
    "        reward = self.model.surface_similarity(vertices, faces)\n",
    "        \n",
    "        if self.illustrate:\n",
    "            illustrate_mesh(vertices, faces).display()\n",
    "        return reward\n",
    "        \n",
    "    def _get_mesh(self, observation):\n",
    "        faces, vertices = poisson_reconstruction(observation.points,\n",
    "                                                 observation.normals,\n",
    "                                                 depth=self._reconstruction_depth)\n",
    "        return vertices, faces\n",
    "\n",
    "    \n",
    "class CombiningObservationsWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self._similarity_threshold = 0.95\n",
    "\n",
    "        self.combined_observation = None\n",
    "        self.plot = None\n",
    "    \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.combined_observation = observation\n",
    "        \n",
    "        if self.env.illustrate:\n",
    "            self.plot = k3d.plot(name='wrapper')\n",
    "            self.plot.display()\n",
    "  \n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "\n",
    "        self._combine_observations(observation)\n",
    "\n",
    "        combined_reward = self.env.step_reward(self.combined_observation)\n",
    "        done = done or combined_reward >= self._similarity_threshold\n",
    "        \n",
    "        new_reward = combined_reward - reward\n",
    "        return self.combined_observation, new_reward, done, info\n",
    "\n",
    "    def render(self, action, observation):\n",
    "        self.plot = self.env.render(action, observation, self.plot)\n",
    "        \n",
    "    def final_reward(self):\n",
    "        return self.env.final_reward(self.combined_observation)\n",
    "    \n",
    "    def _combine_observations(self, observation):\n",
    "        if self.combined_observation is None:\n",
    "            raise EnvError(\"Environment wasn't reset\")\n",
    "        \n",
    "        self.combined_observation += observation\n",
    "\n",
    "\n",
    "class StepPenaltyRewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self._similarity_reward_weight = 1.0\n",
    "            \n",
    "    def reward(self, reward):\n",
    "        reward = -1 + self._similarity_reward_weight * reward\n",
    "        return reward\n",
    "    \n",
    "    def render(self, action, observation):\n",
    "        self.env.render(action, observation)\n",
    "        \n",
    "    def final_reward(self):\n",
    "        return self.env.final_reward()\n",
    "\n",
    "    \n",
    "class DepthMapWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.observation_space = self.observation_space['depth_map']\n",
    "        self.image_size = self.observation_space.shape[1]\n",
    "        self.memory_size = self.observation_space.shape[0]\n",
    "        \n",
    "        self.last_observation = None\n",
    "\n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.last_observation = observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if observation is not None:\n",
    "            self.last_observation = observation\n",
    "            \n",
    "            depth_map = observation.depth_map\n",
    "            assert depth_map.ndim == 3\n",
    "            \n",
    "            new_size = self.memory_size - depth_map.shape[0]\n",
    "            if new_size >= 0:\n",
    "                depth_map = np.pad(depth_map,\n",
    "                                   ((0,new_size),(0,0), (0,0)),\n",
    "                                   mode='constant')\n",
    "            else:\n",
    "                depth_map = depth_map[:self.memory_size]\n",
    "            \n",
    "            return depth_map\n",
    "        return None\n",
    "    \n",
    "    def render(self, action, observation):\n",
    "        if self.last_observation is not None:\n",
    "            self.env.render(action, self.last_observation)\n",
    "    \n",
    "    def final_reward(self):\n",
    "        return self.env.final_reward()\n",
    "\n",
    "        \n",
    "        \n",
    "# TODO one more wrapper for GPU (observation, reward -> tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "        # self.observation_size = env.observation_space.shape[0]\n",
    "        self.actions_cnt = env.action_space.n\n",
    "        \n",
    "        self._max_iter = 20\n",
    "        self._gamma = 0.99\n",
    "        self._final_reward_weight = 1.0\n",
    "  \n",
    "\n",
    "    def predict_action(self, state):\n",
    "        \"\"\"\n",
    "        Return action that should be done from input state according to current policy.\n",
    "        Args:\n",
    "            state: list of points - results of raycasting\n",
    "        return: \n",
    "            action: int\n",
    "        \"\"\"    \n",
    "        # some cool RL staff\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Generate CAD model, reconstruct it and count the reward according   to MSE between original and reconstructed models and number of steps.\n",
    "        Args:\n",
    "            environment: Environment\n",
    "            max_iter: int - max number of iterations to stop (~15)\n",
    "            gamma: float - discounted factor\n",
    "            w: float - weight of mse to final episode reward\n",
    "        return: \n",
    "            episode_reward: float\n",
    "        \"\"\"    \n",
    "        \n",
    "        state = self.env.reset()\n",
    "        \n",
    "        episode_reward = 0.0\n",
    "        states = []\n",
    "        for t in range(self._max_iter):\n",
    "            action = self.predict_action(state)\n",
    "            state, reward, done, info = self.env.step(action)\n",
    "            print(\"REWARD: \", reward)\n",
    "            states.append(state)\n",
    "            self.env.render(action, state)\n",
    "            episode_reward += reward * self._gamma ** t\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        final_reward = self.env.final_reward()\n",
    "        print(\"Hausdorff reward: \", final_reward)\n",
    "        episode_reward += self._final_reward_weight / final_reward # QUESTION\n",
    "        return episode_reward, states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = Environment(illustrate=True)\n",
    "env = CombiningObservationsWrapper(env)\n",
    "env = StepPenaltyRewardWrapper(env)\n",
    "env = DepthMapWrapper(env)\n",
    "\n",
    "agent = RandomAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n",
      "/opt/conda/lib/python3.7/site-packages/traittypes/traittypes.py:101: UserWarning: Given trait value dtype \"int64\" does not match required type \"uint32\". A coerced copy has been created.\n",
      "  np.dtype(self.dtype).name))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20aba69bd7324af7901eb14ff823aa09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabc46b717eb4ff692ec3afbdb491567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REWARD:  -0.8477043215176417\n",
      "REWARD:  -0.6566703462937978\n",
      "REWARD:  -0.6214890224457255\n",
      "REWARD:  -0.39388364201316484\n",
      "REWARD:  -0.4456641726971666\n",
      "REWARD:  -0.34218488082096565\n",
      "REWARD:  -0.4507952083077804\n",
      "REWARD:  -0.42477206754160024\n",
      "REWARD:  -0.4068032217179771\n",
      "REWARD:  -0.33731959605871054\n",
      "REWARD:  -0.31634572141134143\n",
      "REWARD:  -0.39623451490248995\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51130c56538e4c0a9473d6035e8d1512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hausdorff reward:  0.05159408024010445\n"
     ]
    }
   ],
   "source": [
    "reward, states = agent.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f19a3b95cd0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 30000\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * \\\n",
    "        math.exp(-1. * frame_idx / epsilon_decay)\n",
    "\n",
    "plt.plot([epsilon_by_frame(i) for i in range(1000000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.shape[0], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, env.action_space.n)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True)\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    \n",
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), volatile=True)\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].item()\n",
    "            print(action)\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)), volatile=True)\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    q_values      = model(state)\n",
    "    next_q_values = model(next_state)\n",
    "\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss = torch.mean((q_value - expected_q_value.detach()) ** 2) # detach?\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "replay_initial = 10000\n",
    "replay_buffer = ReplayBuffer(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(illustrate=False)\n",
    "env = CombiningObservationsWrapper(env)\n",
    "env = StepPenaltyRewardWrapper(env)\n",
    "env = DepthMapWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n",
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of View Points:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of View Points:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of View Points:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of View Points:  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of View Points:  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of View Points:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of View Points:  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of View Points:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of View Points:  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of View Points:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of View Points:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of View Points:  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of View Points:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of View Points:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of View Points:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to load materials from: FinalBaseMesh.mtl\n",
      "specified material (default)  not loaded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of View Points:  11\n"
     ]
    }
   ],
   "source": [
    "num_frames = 1400000\n",
    "batch_size = 16\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "nof_vp = 0\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = model.act(state, epsilon)\n",
    "\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    nof_vp += 1\n",
    "    \n",
    "    # env.render(action, state)\n",
    "    \n",
    "    if done:\n",
    "#         final_reward = env.final_reward()\n",
    "#         print(\"Number of View Points: \", nof_vp, \" Hausdorff reward: \", final_reward)\n",
    "        print(\"Number of View Points: \", nof_vp)\n",
    "        \n",
    "        state = env.reset()\n",
    "        nof_vp = 0\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        loss = compute_td_loss(batch_size)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    if frame_idx % 10000 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
